{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# =========================================================================== #\n",
      "# Training | Evaluation flags:\n",
      "# =========================================================================== #\n",
      "{'adadelta_rho': 0.95,\n",
      " 'adagrad_initial_accumulator_value': 0.1,\n",
      " 'adam_beta1': 0.9,\n",
      " 'adam_beta2': 0.999,\n",
      " 'batch_size': 32,\n",
      " 'checkpoint_exclude_scopes': 'ssd_300_vgg/conv6,ssd_300_vgg/conv7,ssd_300_vgg/block8,ssd_300_vgg/block9,ssd_300_vgg/block10,ssd_300_vgg/block11,ssd_300_vgg/block4_box,ssd_300_vgg/block7_box,ssd_300_vgg/block8_box,ssd_300_vgg/block9_box,ssd_300_vgg/block10_box,ssd_300_vgg/block11_box',\n",
      " 'checkpoint_model_scope': 'vgg_16',\n",
      " 'checkpoint_path': 'checkpoints/vgg_16.ckpt',\n",
      " 'clone_on_cpu': False,\n",
      " 'dataset_dir': 'tfrecords',\n",
      " 'dataset_name': 'pascalvoc_2007',\n",
      " 'dataset_split_name': 'train',\n",
      " 'end_learning_rate': 0.0001,\n",
      " 'ftrl_initial_accumulator_value': 0.1,\n",
      " 'ftrl_l1': 0.0,\n",
      " 'ftrl_l2': 0.0,\n",
      " 'ftrl_learning_rate_power': -0.5,\n",
      " 'gpu_memory_fraction': 0.8,\n",
      " 'ignore_missing_vars': False,\n",
      " 'label_smoothing': 0.0,\n",
      " 'labels_offset': 0,\n",
      " 'learning_rate': 0.001,\n",
      " 'learning_rate_decay_factor': 0.94,\n",
      " 'learning_rate_decay_type': 'exponential',\n",
      " 'log_every_n_steps': 10,\n",
      " 'loss_alpha': 1.0,\n",
      " 'match_threshold': 0.5,\n",
      " 'max_number_of_steps': None,\n",
      " 'model_name': 'ssd_300_vgg',\n",
      " 'momentum': 0.9,\n",
      " 'moving_average_decay': None,\n",
      " 'negative_ratio': 3.0,\n",
      " 'num_classes': 20,\n",
      " 'num_clones': 1,\n",
      " 'num_epochs_per_decay': 2.0,\n",
      " 'num_preprocessing_threads': 4,\n",
      " 'num_readers': 4,\n",
      " 'opt_epsilon': 1.0,\n",
      " 'optimizer': 'adam',\n",
      " 'preprocessing_name': None,\n",
      " 'rmsprop_decay': 0.9,\n",
      " 'rmsprop_momentum': 0.9,\n",
      " 'save_interval_secs': 600,\n",
      " 'save_summaries_secs': 60,\n",
      " 'train_dir': 'logs/',\n",
      " 'train_image_size': None,\n",
      " 'trainable_scopes': 'ssd_300_vgg/conv6,ssd_300_vgg/conv7,ssd_300_vgg/block8,ssd_300_vgg/block9,ssd_300_vgg/block10,ssd_300_vgg/block11,ssd_300_vgg/block4_box,ssd_300_vgg/block7_box,ssd_300_vgg/block8_box,ssd_300_vgg/block9_box,ssd_300_vgg/block10_box,ssd_300_vgg/block11_box',\n",
      " 'weight_decay': 0.0005}\n",
      "\n",
      "# =========================================================================== #\n",
      "# SSD net parameters:\n",
      "# =========================================================================== #\n",
      "{'anchor_offset': 0.5,\n",
      " 'anchor_ratios': [[2, 0.5],\n",
      "                   [2, 0.5, 3, 0.3333333333333333],\n",
      "                   [2, 0.5, 3, 0.3333333333333333],\n",
      "                   [2, 0.5, 3, 0.3333333333333333],\n",
      "                   [2, 0.5],\n",
      "                   [2, 0.5]],\n",
      " 'anchor_size_bounds': [0.15, 0.9],\n",
      " 'anchor_sizes': [(21.0, 45.0),\n",
      "                  (45.0, 99.0),\n",
      "                  (99.0, 153.0),\n",
      "                  (153.0, 207.0),\n",
      "                  (207.0, 261.0),\n",
      "                  (261.0, 315.0)],\n",
      " 'anchor_steps': [8, 16, 32, 64, 100, 300],\n",
      " 'feat_layers': ['block4', 'block7', 'block8', 'block9', 'block10', 'block11'],\n",
      " 'feat_shapes': [(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],\n",
      " 'img_shape': (300, 300),\n",
      " 'no_annotation_label': 21,\n",
      " 'normalizations': [20, -1, -1, -1, -1, -1],\n",
      " 'num_classes': 20,\n",
      " 'prior_scaling': [0.1, 0.1, 0.2, 0.2]}\n",
      "\n",
      "# =========================================================================== #\n",
      "# Training | Evaluation dataset files:\n",
      "# =========================================================================== #\n",
      "['tfrecords\\\\voc_2007_train_000.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_001.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_002.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_003.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_004.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_005.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_006.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_007.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_008.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_009.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_010.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_011.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_012.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_013.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_014.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_015.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_016.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_017.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_018.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_019.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_020.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_021.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_022.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_023.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_024.tfrecord',\n",
      " 'tfrecords\\\\voc_2007_train_025.tfrecord']\n",
      "\n",
      "INFO:tensorflow:Fine-tuning from checkpoints/vgg_16.ckpt. Ignoring missing vars: False\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/vgg_16.ckpt\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, CPU BiasOp only supports NHWC.\n",
      "\t [[Node: ssd_300_vgg/conv1/conv1_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ssd_300_vgg/conv1/conv1_1/convolution, ssd_300_vgg/conv1/conv1_1/biases/read)]]\n",
      "\n",
      "Caused by op 'ssd_300_vgg/conv1/conv1_1/BiasAdd', defined at:\n",
      "  File \"D:\\Anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"D:\\Anaconda\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 112, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n",
      "    self._run_once()\n",
      "  File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 1425, in _run_once\n",
      "    handle._run()\n",
      "  File \"D:\\Anaconda\\lib\\asyncio\\events.py\", line 127, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 102, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c68c7e9d65aa>\", line 449, in <module>\n",
      "    tf.app.run()\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\n",
      "    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n",
      "  File \"<ipython-input-1-c68c7e9d65aa>\", line 350, in main\n",
      "    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n",
      "  File \"F:\\project\\SSD-Tensorflow-master\\deployment\\model_deploy.py\", line 196, in create_clones\n",
      "    outputs = model_fn(*args, **kwargs)\n",
      "  File \"<ipython-input-1-c68c7e9d65aa>\", line 334, in clone_fn\n",
      "    predictions, localisations, logits, end_points =                     ssd_net.net(b_image, is_training=True)\n",
      "  File \"F:\\project\\SSD-Tensorflow-master\\nets\\ssd_vgg_300.py\", line 155, in net\n",
      "    scope=scope)\n",
      "  File \"F:\\project\\SSD-Tensorflow-master\\nets\\ssd_vgg_300.py\", line 452, in ssd_net\n",
      "    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 1744, in repeat\n",
      "    outputs = layer(outputs, *args, **kwargs)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 181, in func_with_args\n",
      "    return func(*args, **current_args)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 918, in convolution\n",
      "    outputs = layer.apply(inputs)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 320, in apply\n",
      "    return self.__call__(inputs, **kwargs)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 290, in __call__\n",
      "    outputs = self.call(inputs, **kwargs)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 169, in call\n",
      "    data_format=utils.convert_data_format(self.data_format, 4))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1343, in bias_add\n",
      "    return gen_nn_ops._bias_add(value, bias, data_format=data_format, name=name)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 281, in _bias_add\n",
      "    data_format=data_format, name=name)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "InvalidArgumentError (see above for traceback): CPU BiasOp only supports NHWC.\n",
      "\t [[Node: ssd_300_vgg/conv1/conv1_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ssd_300_vgg/conv1/conv1_1/convolution, ssd_300_vgg/conv1/conv1_1/biases/read)]]\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "CPU BiasOp only supports NHWC.\n\t [[Node: ssd_300_vgg/conv1/conv1_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ssd_300_vgg/conv1/conv1_1/convolution, ssd_300_vgg/conv1/conv1_1/biases/read)]]\n\nCaused by op 'ssd_300_vgg/conv1/conv1_1/BiasAdd', defined at:\n  File \"D:\\Anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"D:\\Anaconda\\lib\\asyncio\\events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-c68c7e9d65aa>\", line 449, in <module>\n    tf.app.run()\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"<ipython-input-1-c68c7e9d65aa>\", line 350, in main\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n  File \"F:\\project\\SSD-Tensorflow-master\\deployment\\model_deploy.py\", line 196, in create_clones\n    outputs = model_fn(*args, **kwargs)\n  File \"<ipython-input-1-c68c7e9d65aa>\", line 334, in clone_fn\n    predictions, localisations, logits, end_points =                     ssd_net.net(b_image, is_training=True)\n  File \"F:\\project\\SSD-Tensorflow-master\\nets\\ssd_vgg_300.py\", line 155, in net\n    scope=scope)\n  File \"F:\\project\\SSD-Tensorflow-master\\nets\\ssd_vgg_300.py\", line 452, in ssd_net\n    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 1744, in repeat\n    outputs = layer(outputs, *args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 918, in convolution\n    outputs = layer.apply(inputs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 290, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 169, in call\n    data_format=utils.convert_data_format(self.data_format, 4))\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1343, in bias_add\n    return gen_nn_ops._bias_add(value, bias, data_format=data_format, name=name)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 281, in _bias_add\n    data_format=data_format, name=name)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): CPU BiasOp only supports NHWC.\n\t [[Node: ssd_300_vgg/conv1/conv1_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ssd_300_vgg/conv1/conv1_1/convolution, ssd_300_vgg/conv1/conv1_1/biases/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: CPU BiasOp only supports NHWC.\n\t [[Node: ssd_300_vgg/conv1/conv1_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ssd_300_vgg/conv1/conv1_1/convolution, ssd_300_vgg/conv1/conv1_1/biases/read)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[1;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[0;32m    949\u001b[0m           start_standard_services=start_standard_services)\n\u001b[1;32m--> 950\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\learning.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_op, logdir, train_step_fn, train_step_kwargs, log_every_n_steps, graph, master, is_chief, global_step, number_of_steps, init_op, init_feed_dict, local_init_op, init_fn, ready_op, summary_op, save_summaries_secs, summary_writer, startup_delay_steps, saver, save_interval_secs, sync_optimizer, session_config, trace_every_n_steps)\u001b[0m\n\u001b[0;32m    741\u001b[0m             total_loss, should_stop = train_step_fn(\n\u001b[1;32m--> 742\u001b[1;33m                 sess, train_op, global_step, train_step_kwargs)\n\u001b[0m\u001b[0;32m    743\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\learning.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(sess, train_op, global_step, train_step_kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m                                         \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrace_run_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m                                         run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    485\u001b[0m   \u001b[0mtime_elapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: CPU BiasOp only supports NHWC.\n\t [[Node: ssd_300_vgg/conv1/conv1_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ssd_300_vgg/conv1/conv1_1/convolution, ssd_300_vgg/conv1/conv1_1/biases/read)]]\n\nCaused by op 'ssd_300_vgg/conv1/conv1_1/BiasAdd', defined at:\n  File \"D:\\Anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"D:\\Anaconda\\lib\\asyncio\\events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-c68c7e9d65aa>\", line 449, in <module>\n    tf.app.run()\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"<ipython-input-1-c68c7e9d65aa>\", line 350, in main\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n  File \"F:\\project\\SSD-Tensorflow-master\\deployment\\model_deploy.py\", line 196, in create_clones\n    outputs = model_fn(*args, **kwargs)\n  File \"<ipython-input-1-c68c7e9d65aa>\", line 334, in clone_fn\n    predictions, localisations, logits, end_points =                     ssd_net.net(b_image, is_training=True)\n  File \"F:\\project\\SSD-Tensorflow-master\\nets\\ssd_vgg_300.py\", line 155, in net\n    scope=scope)\n  File \"F:\\project\\SSD-Tensorflow-master\\nets\\ssd_vgg_300.py\", line 452, in ssd_net\n    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 1744, in repeat\n    outputs = layer(outputs, *args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 918, in convolution\n    outputs = layer.apply(inputs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 290, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 169, in call\n    data_format=utils.convert_data_format(self.data_format, 4))\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1343, in bias_add\n    return gen_nn_ops._bias_add(value, bias, data_format=data_format, name=name)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 281, in _bias_add\n    data_format=data_format, name=name)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): CPU BiasOp only supports NHWC.\n\t [[Node: ssd_300_vgg/conv1/conv1_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ssd_300_vgg/conv1/conv1_1/convolution, ssd_300_vgg/conv1/conv1_1/biases/read)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c68c7e9d65aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     46\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-c68c7e9d65aa>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[0msave_interval_secs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_interval_secs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[0msession_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             sync_optimizer=None)\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\learning.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_op, logdir, train_step_fn, train_step_kwargs, log_every_n_steps, graph, master, is_chief, global_step, number_of_steps, init_op, init_feed_dict, local_init_op, init_fn, ready_op, summary_op, save_summaries_secs, summary_writer, startup_delay_steps, saver, save_interval_secs, sync_optimizer, session_config, trace_every_n_steps)\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogdir\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_chief\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m           \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Finished training! Saving model to disk.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m           \u001b[0msv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAbortedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[1;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[1;31m# threads which are not checking for `should_stop()`.  They\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m         \u001b[1;31m# will be stopped when we close the session further down.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclose_summary_writer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose_summary_writer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    961\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;31m# Close the session to finish up all pending calls.  We do not care\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\u001b[0m in \u001b[0;36mstop\u001b[1;34m(self, threads, close_summary_writer)\u001b[0m\n\u001b[0;32m    786\u001b[0m       \u001b[1;31m# reported.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m       self._coord.join(threads,\n\u001b[1;32m--> 788\u001b[1;33m                        stop_grace_period_secs=self._stop_grace_secs)\n\u001b[0m\u001b[0;32m    789\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m       \u001b[1;31m# Close the writer last, in case one of the running threads was using it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[0;32m    387\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_registered_threads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mstragglers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mignore_live_threads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\u001b[0m in \u001b[0;36mstop_on_exception\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m       \u001b[1;32myield\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_timer_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m           \u001b[0mnext_timer_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timer_interval_secs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\u001b[0m in \u001b[0;36mrun_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m       summary_strs, global_step = self._sess.run([self._sv.summary_op,\n\u001b[1;32m--> 990\u001b[1;33m                                                   self._sv.global_step])\n\u001b[0m\u001b[0;32m    991\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[0msummary_strs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: CPU BiasOp only supports NHWC.\n\t [[Node: ssd_300_vgg/conv1/conv1_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ssd_300_vgg/conv1/conv1_1/convolution, ssd_300_vgg/conv1/conv1_1/biases/read)]]\n\nCaused by op 'ssd_300_vgg/conv1/conv1_1/BiasAdd', defined at:\n  File \"D:\\Anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"D:\\Anaconda\\lib\\asyncio\\events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-c68c7e9d65aa>\", line 449, in <module>\n    tf.app.run()\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"<ipython-input-1-c68c7e9d65aa>\", line 350, in main\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n  File \"F:\\project\\SSD-Tensorflow-master\\deployment\\model_deploy.py\", line 196, in create_clones\n    outputs = model_fn(*args, **kwargs)\n  File \"<ipython-input-1-c68c7e9d65aa>\", line 334, in clone_fn\n    predictions, localisations, logits, end_points =                     ssd_net.net(b_image, is_training=True)\n  File \"F:\\project\\SSD-Tensorflow-master\\nets\\ssd_vgg_300.py\", line 155, in net\n    scope=scope)\n  File \"F:\\project\\SSD-Tensorflow-master\\nets\\ssd_vgg_300.py\", line 452, in ssd_net\n    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 1744, in repeat\n    outputs = layer(outputs, *args, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 918, in convolution\n    outputs = layer.apply(inputs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 290, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 169, in call\n    data_format=utils.convert_data_format(self.data_format, 4))\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1343, in bias_add\n    return gen_nn_ops._bias_add(value, bias, data_format=data_format, name=name)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 281, in _bias_add\n    data_format=data_format, name=name)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): CPU BiasOp only supports NHWC.\n\t [[Node: ssd_300_vgg/conv1/conv1_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ssd_300_vgg/conv1/conv1_1/convolution, ssd_300_vgg/conv1/conv1_1/biases/read)]]\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2016 Paul Balanca. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Generic training script that trains a SSD model using a given dataset.\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "\n",
    "from datasets import dataset_factory\n",
    "from deployment import model_deploy\n",
    "from nets import nets_factory\n",
    "from preprocessing import preprocessing_factory\n",
    "import tf_utils\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "DATA_FORMAT = 'NCHW'\n",
    "\n",
    "DATASET_DIR = 'tfrecords'\n",
    "TRAIN_DIR = 'logs/'\n",
    "CHECKPOINT_PATH = 'checkpoints/vgg_16.ckpt'\n",
    "# =========================================================================== #\n",
    "# SSD Network flags.\n",
    "# =========================================================================== #\n",
    "#loss_alpha：学习率\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'loss_alpha', 1., 'Alpha parameter in the loss function.')\n",
    "#negative_ratio：负样本数量与正样本的比率\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'negative_ratio', 3., 'Negative ratio in the loss function.')\n",
    "#预测框与真实框匹配的阈值（ IOU>match_threshold 作为正样本）\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'match_threshold', 0.5, 'Matching threshold in the loss function.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# General Flags.\n",
    "# =========================================================================== #\n",
    "#训练数据的位置\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'train_dir', TRAIN_DIR,\n",
    "    'Directory where checkpoints and event logs are written to.')\n",
    "\"\"\"\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'train_dir', '/tmp/tfmodel/',\n",
    "    'Directory where checkpoints and event logs are written to.')\n",
    "\"\"\"\n",
    "#设置单机多GPU的参数\n",
    "tf.app.flags.DEFINE_integer('num_clones', 1,\n",
    "                            'Number of model clones to deploy.')\n",
    "#不使用cpu\n",
    "tf.app.flags.DEFINE_boolean('clone_on_cpu', False,\n",
    "                            'Use CPUs to deploy clones.')\n",
    "#同时读取数据的readers数目\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_readers', 4,\n",
    "    'The number of parallel readers that read data from the dataset.')\n",
    "#预处理中使用的线程数\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_preprocessing_threads', 4,\n",
    "    'The number of threads used to create the batches.')\n",
    "#每十步输出一次日志\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'log_every_n_steps', 10,\n",
    "    'The frequency with which logs are print.')\n",
    "#保存模型的summaries，用于可视化显示\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_summaries_secs', 60,\n",
    "    'The frequency with which summaries are saved, in seconds.')\n",
    "#保存模型的时间间隔\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_interval_secs', 600,\n",
    "    'The frequency with which the model is saved, in seconds.')\n",
    "\"\"\"\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_summaries_secs', 600,\n",
    "    'The frequency with which summaries are saved, in seconds.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_interval_secs', 600,\n",
    "    'The frequency with which the model is saved, in seconds.')\n",
    "\"\"\"\n",
    "#GPU利用率\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'gpu_memory_fraction', 0.8, 'GPU memory fraction to use.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Optimization Flags.\n",
    "# =========================================================================== #\n",
    "#设置权重衰减\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'weight_decay', 0.0005, 'The weight decay on the model weights.')\n",
    "#设置优化方法\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'optimizer', 'adam',\n",
    "    'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\n",
    "    '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\n",
    "\"\"\"\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'weight_decay', 0.00004, 'The weight decay on the model weights.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'optimizer', 'rmsprop',\n",
    "    'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\n",
    "    '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\n",
    "\"\"\"\n",
    "#设置优化器的参数\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adadelta_rho', 0.95,\n",
    "    'The decay rate for adadelta.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adagrad_initial_accumulator_value', 0.1,\n",
    "    'Starting value for the AdaGrad accumulators.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adam_beta1', 0.9,\n",
    "    'The exponential decay rate for the 1st moment estimates.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adam_beta2', 0.999,\n",
    "    'The exponential decay rate for the 2nd moment estimates.')\n",
    "tf.app.flags.DEFINE_float('opt_epsilon', 1.0, 'Epsilon term for the optimizer.')\n",
    "tf.app.flags.DEFINE_float('ftrl_learning_rate_power', -0.5,\n",
    "                          'The learning rate power.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_initial_accumulator_value', 0.1,\n",
    "    'Starting value for the FTRL accumulators.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_l1', 0.0, 'The FTRL l1 regularization strength.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_l2', 0.0, 'The FTRL l2 regularization strength.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'momentum', 0.9,\n",
    "    'The momentum for the MomentumOptimizer and RMSPropOptimizer.')\n",
    "tf.app.flags.DEFINE_float('rmsprop_momentum', 0.9, 'Momentum.')\n",
    "tf.app.flags.DEFINE_float('rmsprop_decay', 0.9, 'Decay term for RMSProp.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Learning Rate Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'learning_rate_decay_type',\n",
    "    'exponential',\n",
    "    'Specifies how the learning rate is decayed. One of \"fixed\", \"exponential\",'\n",
    "    ' or \"polynomial\"')\n",
    "\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "\"\"\"\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "\"\"\"\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'end_learning_rate', 0.0001,\n",
    "    'The minimal end learning rate used by a polynomial decay learning rate.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'label_smoothing', 0.0, 'The amount of label smoothing.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'learning_rate_decay_factor', 0.94, 'Learning rate decay factor.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'num_epochs_per_decay', 2.0,\n",
    "    'Number of epochs after which learning rate decays.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'moving_average_decay', None,\n",
    "    'The decay to use for the moving average.'\n",
    "    'If left as None, then moving averages are not used.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Dataset Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_name', 'pascalvoc_2007', 'The name of the dataset to load.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_classes', 20, 'Number of classes to use in the dataset.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_split_name', 'train', 'The name of the train/test split.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_dir', DATASET_DIR, 'The directory where the dataset files are stored.')\n",
    "\"\"\" \n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_name', 'imagenet', 'The name of the dataset to load.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_classes', 21, 'Number of classes to use in the dataset.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_split_name', 'train', 'The name of the train/test split.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_dir', None, 'The directory where the dataset files are stored.')\n",
    "\"\"\"\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'labels_offset', 0,\n",
    "    'An offset for the labels in the dataset. This flag is primarily used to '\n",
    "    'evaluate the VGG and ResNet architectures which do not use a background '\n",
    "    'class for the ImageNet dataset.')\n",
    "#需要用到的模型\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'model_name', 'ssd_300_vgg', 'The name of the architecture to train.')\n",
    "#用到的预处理方法\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'preprocessing_name', None, 'The name of the preprocessing to use. If left '\n",
    "    'as `None`, then the model_name flag is used.')\n",
    "\"\"\"\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'batch_size', 16, 'The number of samples in each batch.')\n",
    "\"\"\"\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'batch_size', 32, 'The number of samples in each batch.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'train_image_size', None, 'Train image size')\n",
    "tf.app.flags.DEFINE_integer('max_number_of_steps', None,\n",
    "                            'The maximum number of training steps.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Fine-Tuning Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_path', CHECKPOINT_PATH,\n",
    "    'The path to a checkpoint from which to fine-tune.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_model_scope', 'vgg_16',\n",
    "    'Model scope in the checkpoint. None if the same as the trained model.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_exclude_scopes', 'ssd_300_vgg/conv6,ssd_300_vgg/conv7,ssd_300_vgg/block8,ssd_300_vgg/block9,ssd_300_vgg/block10,ssd_300_vgg/block11,ssd_300_vgg/block4_box,ssd_300_vgg/block7_box,ssd_300_vgg/block8_box,ssd_300_vgg/block9_box,ssd_300_vgg/block10_box,ssd_300_vgg/block11_box',\n",
    "    'Comma-separated list of scopes of variables to exclude when restoring '\n",
    "    'from a checkpoint.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'trainable_scopes', 'ssd_300_vgg/conv6,ssd_300_vgg/conv7,ssd_300_vgg/block8,ssd_300_vgg/block9,ssd_300_vgg/block10,ssd_300_vgg/block11,ssd_300_vgg/block4_box,ssd_300_vgg/block7_box,ssd_300_vgg/block8_box,ssd_300_vgg/block9_box,ssd_300_vgg/block10_box,ssd_300_vgg/block11_box',\n",
    "    'Comma-separated list of scopes to filter the set of variables to train.'\n",
    "    'By default, None would train all the variables.')\n",
    "\"\"\"\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_path', None,\n",
    "    'The path to a checkpoint from which to fine-tune.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_model_scope', None,\n",
    "    'Model scope in the checkpoint. None if the same as the trained model.')   \n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_exclude_scopes', None,\n",
    "    'Comma-separated list of scopes of variables to exclude when restoring '\n",
    "    'from a checkpoint.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'trainable_scopes', None,\n",
    "    'Comma-separated list of scopes to filter the set of variables to train.'\n",
    "    'By default, None would train all the variables.')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_boolean(\n",
    "    'ignore_missing_vars', False,\n",
    "    'When restoring a checkpoint would ignore missing variables.')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "# =========================================================================== #\n",
    "# Main training routine.\n",
    "# =========================================================================== #\n",
    "def main(_):\n",
    "    #查看数据集位置是否成功设置\n",
    "    if not FLAGS.dataset_dir:\n",
    "        raise ValueError('You must supply the dataset directory with --dataset_dir')\n",
    "    #设置日志记录方式：DEBUG\n",
    "    tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "    with tf.Graph().as_default():\n",
    "        # Config model_deploy. Keep TF Slim Models structure.\n",
    "        # Useful if want to need multiple GPUs and/or servers in the future.\n",
    "    #对cpu\\gpu进行设置：用到的gpu块数，是否用cpu，replica的id，replica的个数，ps任务数等\n",
    "        deploy_config = model_deploy.DeploymentConfig(\n",
    "            num_clones=FLAGS.num_clones,\n",
    "            clone_on_cpu=FLAGS.clone_on_cpu,\n",
    "            replica_id=0,\n",
    "            num_replicas=1,\n",
    "            num_ps_tasks=0)\n",
    "        # global_step：每次learning rate变化的时候自动更新\n",
    "        with tf.device(deploy_config.variables_device()):\n",
    "            global_step = slim.create_global_step()\n",
    "\n",
    "    # Select the dataset.\n",
    "        #pascalvoc_2007, train, rf_records\n",
    "        dataset = dataset_factory.get_dataset(\n",
    "            FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n",
    "\n",
    "    # Get the SSD network and its anchors.\n",
    "        #ssd_class = ssd_vgg_300.SSDNet\n",
    "        ssd_class = nets_factory.get_network(FLAGS.model_name)\n",
    "        #对SSDNet的参数进行修改（num_classes）\n",
    "        ssd_params = ssd_class.default_params._replace(num_classes=FLAGS.num_classes)\n",
    "        #ssd_net = 新的ssd_vgg_300.SSDNet\n",
    "        ssd_net = ssd_class(ssd_params)\n",
    "        #ssd_shape = SSDNet默认输入的图像大小\n",
    "        ssd_shape = ssd_net.params.img_shape\n",
    "        #生成图像所有的anchors，保存其坐标：[Y,X,H,W]\n",
    "        ssd_anchors = ssd_net.anchors(ssd_shape)\n",
    "\n",
    "    # Select the preprocessing function.\n",
    "        #preprocessing_name = ssd_vgg_300\n",
    "        preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n",
    "        #image_preprocessing_fn = ssd_vgg_preprocessing.preprocess_image\n",
    "        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n",
    "            preprocessing_name, is_training=True)\n",
    "\n",
    "        tf_utils.print_configuration(FLAGS.__flags, ssd_params,\n",
    "                                     dataset.data_sources, FLAGS.train_dir)\n",
    "        # =================================================================== #\n",
    "        # Create a dataset provider and batches.\n",
    "        # =================================================================== #\n",
    "        with tf.device(deploy_config.inputs_device()):\n",
    "            #设置DatasetDataProvider:dataset,num_readers=4,队列容量，队列最小值，shuffle\n",
    "            #multiple readers use a ParallelReader to read from multiple files in parallel\n",
    "            #common_queue_capacity:The upper bound on the number of elements that may be stored in this queue\n",
    "            #common_queue_min=min_after_dequeue\n",
    "            with tf.name_scope(FLAGS.dataset_name + '_data_provider'):\n",
    "                provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "                    dataset,\n",
    "                    num_readers=FLAGS.num_readers,\n",
    "                    common_queue_capacity=20 * FLAGS.batch_size,\n",
    "                    common_queue_min=10 * FLAGS.batch_size,\n",
    "                    shuffle=True)\n",
    "            # Get for SSD network: image, labels, bboxes.\n",
    "            #从provider中取出数据image，shape，labels, bboxes.\n",
    "            #provider.get:_items_to_tensors[item] for item in items\n",
    "            #一次得到10 * batch_size条数据\n",
    "            [image, shape, glabels, gbboxes] = provider.get(['image', 'shape',\n",
    "                                                             'object/label',\n",
    "                                                             'object/bbox'])\n",
    "            # Pre-processing image, labels and bboxes.\n",
    "            #对图像进行预处理，得到image, glabels, gbboxes\n",
    "            image, glabels, gbboxes = \\\n",
    "                image_preprocessing_fn(image, glabels, gbboxes,\n",
    "                                       out_shape=ssd_shape,\n",
    "                                       data_format=DATA_FORMAT)\n",
    "            # Encode groundtruth labels and bboxes.\n",
    "            #对glabels, gbboxes, ssd_anchors进行编码\n",
    "            #通过编码，得到训练数据的gclasses, glocalisations, gscores\n",
    "            gclasses, glocalisations, gscores = \\\n",
    "                ssd_net.bboxes_encode(glabels, gbboxes, ssd_anchors)\n",
    "            #batch_shape = [1,6,6,6]   6:特征图层数\n",
    "            batch_shape = [1] + [len(ssd_anchors)] * 3\n",
    "\n",
    "            #得到Training batches:输入data,batch_size,num_threads,capacity\n",
    "            #Tensor with shape [x, y, z] will be output with shape [batch_size, x, y, z].\n",
    "            r = tf.train.batch(\n",
    "                tf_utils.reshape_list([image, gclasses, glocalisations, gscores]),\n",
    "                batch_size=FLAGS.batch_size,\n",
    "                num_threads=FLAGS.num_preprocessing_threads,\n",
    "                capacity=5 * FLAGS.batch_size)\n",
    "            #对Training batches的shape进行整理\n",
    "            b_image, b_gclasses, b_glocalisations, b_gscores = \\\n",
    "                tf_utils.reshape_list(r, batch_shape)\n",
    "\n",
    "            # Intermediate queueing: unique batch computation pipeline for all\n",
    "            # GPUs running the training.\n",
    "            #batch_queue:预加载队列\n",
    "            batch_queue = slim.prefetch_queue.prefetch_queue(\n",
    "                tf_utils.reshape_list([b_image, b_gclasses, b_glocalisations, b_gscores]),\n",
    "                capacity=2 * deploy_config.num_clones)\n",
    "\n",
    "        # ================================= ================================== #\n",
    "        # Define the model running on every GPU.\n",
    "        # =================================================================== #\n",
    "        def clone_fn(batch_queue):\n",
    "            \"\"\"Allows data parallelism by creating multiple\n",
    "            clones of network_fn.\"\"\"\n",
    "            # Dequeue batch.\n",
    "            #Decode和Dequeue后训练数据每个anchor的gclasses, glocalisations, gscores\n",
    "            b_image, b_gclasses, b_glocalisations, b_gscores = \\\n",
    "                tf_utils.reshape_list(batch_queue.dequeue(), batch_shape)\n",
    "\n",
    "            # Construct SSD network.\n",
    "            #预设置weight_decay和data_format参数\n",
    "            arg_scope = ssd_net.arg_scope(weight_decay=FLAGS.weight_decay,\n",
    "                                          data_format=DATA_FORMAT)\n",
    "            with slim.arg_scope(arg_scope):\n",
    "            #返回预测的类别置信度、位置偏移量、类别得分、特征图\n",
    "                predictions, localisations, logits, end_points = \\\n",
    "                    ssd_net.net(b_image, is_training=True)\n",
    "            # Add loss function.\n",
    "            #损失函数\n",
    "            ssd_net.losses(logits, localisations,\n",
    "                           b_gclasses, b_glocalisations, b_gscores,\n",
    "                           match_threshold=FLAGS.match_threshold,\n",
    "                           negative_ratio=FLAGS.negative_ratio,\n",
    "                           alpha=FLAGS.loss_alpha,\n",
    "                           label_smoothing=FLAGS.label_smoothing)\n",
    "            return end_points\n",
    "\n",
    "        # Gather initial summaries.\n",
    "        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Add summaries from first clone.\n",
    "        # =================================================================== #\n",
    "        #制作clones：Clone(clone_fn(batch_queue)), clone_scope, clone_device)\n",
    "        clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n",
    "        first_clone_scope = deploy_config.clone_scope(0)\n",
    "        # Gather update_ops from the first clone. These contain, for example,\n",
    "        # the updates for the batch_norm variables created by network_fn.\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n",
    "\n",
    "        # Add summaries for end_points.\n",
    "        end_points = clones[0].outputs\n",
    "        for end_point in end_points:\n",
    "            x = end_points[end_point]\n",
    "            summaries.add(tf.summary.histogram('activations/' + end_point, x))\n",
    "            summaries.add(tf.summary.scalar('sparsity/' + end_point,\n",
    "                                            tf.nn.zero_fraction(x)))\n",
    "        # Add summaries for losses and extra losses.\n",
    "        for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n",
    "            summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "        for loss in tf.get_collection('EXTRA_LOSSES', first_clone_scope):\n",
    "            summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "        # Add summaries for variables.\n",
    "        for variable in slim.get_model_variables():\n",
    "            summaries.add(tf.summary.histogram(variable.op.name, variable))\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Configure the moving averages.\n",
    "        # =================================================================== #\n",
    "        if FLAGS.moving_average_decay:\n",
    "            moving_average_variables = slim.get_model_variables()\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(\n",
    "                FLAGS.moving_average_decay, global_step)\n",
    "        else:\n",
    "            moving_average_variables, variable_averages = None, None\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Configure the optimization procedure.\n",
    "        # =================================================================== #\n",
    "        with tf.device(deploy_config.optimizer_device()):\n",
    "            #tf.train.exponential_decay()\n",
    "            learning_rate = tf_utils.configure_learning_rate(FLAGS,\n",
    "                                                             dataset.num_samples,\n",
    "                                                             global_step)\n",
    "            #设置优化器的学习率，并配置优化器 optimizer = tf.train.AdamOptimizer(learning_rate,\n",
    "            #     beta1=flags.adam_beta1,beta2=flags.adam_beta2,epsilon=flags.opt_epsilon)\n",
    "            optimizer = tf_utils.configure_optimizer(FLAGS, learning_rate)\n",
    "            summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "        if FLAGS.moving_average_decay:\n",
    "            # Update ops executed locally by trainer.\n",
    "            update_ops.append(variable_averages.apply(moving_average_variables))\n",
    "\n",
    "        # Variables to train.\n",
    "        #选择需要更新的参数\n",
    "        variables_to_train = tf_utils.get_variables_to_train(FLAGS)\n",
    "\n",
    "        # and returns a train_tensor and summary_op\n",
    "        #计算各个clones的loss之和，并使用优化器优化loss之和，并通过compute_gradients()根据loss得到梯度\n",
    "        #注：常用的tf.train.Optimizer.minimize是简单的合并了compute_gradients()与apply_gradients()函数\n",
    "        total_loss, clones_gradients = model_deploy.optimize_clones(\n",
    "            clones,\n",
    "            optimizer,\n",
    "            var_list=variables_to_train)\n",
    "        # Add total_loss to summary.\n",
    "        summaries.add(tf.summary.scalar('total_loss', total_loss))\n",
    "\n",
    "        # Create gradient updates.apply_gradients()函数\n",
    "        # global_step: Optional Variable to increment by one after the variables have been updated.\n",
    "        grad_updates = optimizer.apply_gradients(clones_gradients,\n",
    "                                                 global_step=global_step)\n",
    "        update_ops.append(grad_updates)\n",
    "        ## tf.group()将多个tensor或者op合在一起，然后进行run，返回的是一个op\n",
    "        update_op = tf.group(*update_ops)\n",
    "        #计算训练的步骤（通过total_loss和optimizer得到）\n",
    "        #with_dependencies(dependencies, output_tensor, name=None)\n",
    "        #Produces the content of `output_tensor` only after `dependencies`.\n",
    "        #train_tensor：train_ops:在update_op完成后，计算total_loss并返回\n",
    "        train_tensor = control_flow_ops.with_dependencies([update_op], total_loss,\n",
    "                                                          name='train_op')\n",
    "\n",
    "        # Add the summaries from the first clone. These contain the summaries\n",
    "        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n",
    "                                           first_clone_scope))\n",
    "        # Merge all summaries together.\n",
    "        summary_op = tf.summary.merge(list(summaries), name='summary_op')\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Kicks off the training.\n",
    "        # =================================================================== #\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n",
    "        config = tf.ConfigProto(log_device_placement=False,\n",
    "                                gpu_options=gpu_options)\n",
    "        saver = tf.train.Saver(max_to_keep=5,\n",
    "                               keep_checkpoint_every_n_hours=1.0,\n",
    "                               write_version=2,\n",
    "                               pad_step_number=False)\n",
    "        #反复测量损失，计算梯度并将模型保存到磁盘\n",
    "        slim.learning.train(\n",
    "            train_tensor,\n",
    "            logdir=FLAGS.train_dir,\n",
    "            master='',\n",
    "            is_chief=True,\n",
    "            init_fn=tf_utils.get_init_fn(FLAGS),\n",
    "            summary_op=summary_op,\n",
    "            number_of_steps=FLAGS.max_number_of_steps,\n",
    "            log_every_n_steps=FLAGS.log_every_n_steps,\n",
    "            save_summaries_secs=FLAGS.save_summaries_secs,\n",
    "            saver=saver,\n",
    "            save_interval_secs=FLAGS.save_interval_secs,\n",
    "            session_config=config,\n",
    "            sync_optimizer=None)\n",
    "\n",
    "#处理flag解析，然后执行main函数\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
